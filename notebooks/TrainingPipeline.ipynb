{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(file_path):\n",
    "    \n",
    "    with open(file_path, \"rb\") as file_obj:\n",
    "        return pickle.load(file_obj)\n",
    "\n",
    "filepath = \"./models/\"\n",
    "\n",
    "RF_pre_trained_model = load_object(filepath + f'RandomForestRegressorModel.pkl')\n",
    "XGB_pre_trained_model = load_object(filepath + f'XGRegressorModel_v2.pkl')\n",
    "preprocessor = load_object(filepath + f'preprocessor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(file_path, obj):\n",
    "    \n",
    "    dir_path = os.path.dirname(file_path)\n",
    "\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    with open(file_path, \"wb\") as file_obj:\n",
    "        pickle.dump(obj, file_obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "\n",
    "    # Calculate the beta and alpha values\n",
    "    df['beta'] = df['depth'] / 100\n",
    "    df['alpha'] = (1 - df['beta']) * (1 + (df['table'] / 100)**2)\n",
    "\n",
    "    # Calculate the volume of the diamond\n",
    "    df['volume'] = 0.5 * df['z'] * df['x'] * df['y'] * (df['alpha'] + df['beta'])\n",
    "\n",
    "    # Calculate the density of the diamond\n",
    "    df['density'] = df['carat'] / df['volume']\n",
    "\n",
    "    # Drop the auxiliary columns\n",
    "    df.drop(['beta', 'alpha'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_outliers(df):\n",
    "    \n",
    "    # Define the conditions for removing outliers\n",
    "    conditions = [\n",
    "        (df['carat'] > 0) & (df['price'] < 100),\n",
    "        (df['z'] > 2) & (df['price'] < 100),\n",
    "        (df['z'] < 2),\n",
    "        (df['y'] > 3) & (df['price'] < 100),\n",
    "        (df['y'] < 2),\n",
    "        (df['x'] > 2) & (df['price'] < 100),\n",
    "        (df['x'] < 2),\n",
    "        (df['table'] > 75),\n",
    "        (df['depth'] < 50),\n",
    "        (df['density'] < 0.008)\n",
    "\n",
    "    ]\n",
    "\n",
    "    # Create a mask for the rows to be removed\n",
    "    mask = np.any(conditions, axis=0)\n",
    "\n",
    "    # Drop the rows that meet the conditions\n",
    "    df = df[~mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_features(df,redundant_features = ['x', 'y', 'z', 'density']):\n",
    "    \n",
    "    df = df.drop(redundant_features, axis=1)\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df,preprocessor,\n",
    "                    numeric_features = ['volume', 'carat', 'depth', 'table'],\n",
    "                    categorical_features = ['color', 'cut', 'clarity'],\n",
    "                    target = 'price'):\n",
    "\n",
    "    # Adding Features\n",
    "    df = feature_engineering(df)\n",
    "\n",
    "    # Removing Outliers\n",
    "    df = removing_outliers(df)\n",
    "\n",
    "    # Drop redundant features\n",
    "    df = drop_redundant_features(df) \n",
    "\n",
    "    # Preprocess the data\n",
    "    X_new = df.drop(target, axis=1)\n",
    "    y_new = df[target]\n",
    "    X_new_preprocessed = preprocessor.transform(X_new)\n",
    "\n",
    "    return X_new_preprocessed , y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#  Fine-tune the XGBoost model on the new data\\ndnew = xgb.DMatrix(X_new_preprocessed, label=y_new)\\n\\n# Update the model by training on new data\\nparams = XGB_pre_trained_model.get_xgb_params()\\nupdate_model = xgb.train(  params, dnew,\\n                                   num_boost_round=XGB_pre_trained_model.get_num_boosting_rounds(),\\n                                     xgb_model=XGB_pre_trained_model)\\n\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = ['volume', 'carat', 'depth', 'table']\n",
    "categorical_features = ['color', 'cut', 'clarity']\n",
    "target = 'price'\n",
    "\n",
    "\n",
    "# Load the new data\n",
    "filepath = '../datasets/diamonds/'\n",
    "fresh_data = pd.read_csv(filepath + 'fresh_data.csv')\n",
    "\n",
    "# Process data\n",
    "X_new_preprocessed , y_new = preprocess_data(   df = fresh_data,\n",
    "                                                preprocessor = preprocessor,\n",
    "                                                numeric_features = numeric_features,\n",
    "                                                categorical_features = categorical_features,\n",
    "                                                target = target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune the model on the fresh data\n",
    "\n",
    "In this case, only the XGRegressor model is partially fit over the fresh data, as the RandomForest Regressor is not compatible with partial fitting.\n",
    "\n",
    "NOTE: In case, it is really needed to incrementally train the RandomForest Regressor model on the new data, there exist the posibility of employing [alternative libraries]() which expand the posibilities of some regressor, allowing to train them incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [20:02:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"nrounds\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Fine-tune the XGBoost model on the new data\n",
    "dnew = xgb.DMatrix(X_new_preprocessed, label=y_new)\n",
    "\n",
    "# Update the model by training on new data\n",
    "#params = XGB_pre_trained_model.get_xgb_params()\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'eta': 0.1,\n",
    "}\n",
    "# Set the number of boosting rounds to 0 to perform partial training\n",
    "params['nrounds'] = 0\n",
    "\n",
    "# Update the existing model\n",
    "updated_model = xgb.train(  params, dnew, xgb_model=XGB_pre_trained_model, num_boost_round=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics:\n",
      "Mean Absolute Error (MAE): 674.95\n",
      "Mean Squared Error (MSE): 2170689.76\n",
      "Root Mean Squared Error (RMSE): 1473.33\n",
      "R-squared (R2): 0.92\n",
      "\n",
      "XGBoost Metrics:\n",
      "Mean Absolute Error (MAE): 514.18\n",
      "Mean Squared Error (MSE): 2214855.52\n",
      "Root Mean Squared Error (RMSE): 1488.24\n",
      "R-squared (R2): 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Predictions on the test set\n",
    "rf_predictions = RF_pre_trained_model.predict(X_new_preprocessed)\n",
    "xgb_predictions = updated_model.predict(dnew)\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "rf_mae = mean_absolute_error(y_new, rf_predictions)\n",
    "rf_mse = mean_squared_error(y_new, rf_predictions)\n",
    "rf_rmse = mean_squared_error(y_new, rf_predictions, squared=False)\n",
    "rf_r2 = r2_score(y_new, rf_predictions)\n",
    "\n",
    "print(\"Random Forest Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {rf_mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {rf_mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rf_rmse:.2f}\")\n",
    "print(f\"R-squared (R2): {rf_r2:.2f}\")\n",
    "print()\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "xgb_mae = mean_absolute_error(y_new, xgb_predictions)\n",
    "xgb_mse = mean_squared_error(y_new, xgb_predictions)\n",
    "xgb_rmse = mean_squared_error(y_new, xgb_predictions, squared=False)\n",
    "xgb_r2 = r2_score(y_new, xgb_predictions)\n",
    "\n",
    "print(\"XGBoost Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {xgb_mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {xgb_mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {xgb_rmse:.2f}\")\n",
    "print(f\"R-squared (R2): {xgb_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save regressors\n",
    "filepath = \"./models/\"\n",
    "save_object(filepath + f'XGRegressorModel_v2.pkl', updated_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Base version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Define the model and preprocessor paths\n",
    "model_path = \"./models/\"\n",
    "preprocessor_path = \"./models/preprocessor.pkl\"\n",
    "\n",
    "def train_pipeline():\n",
    "    # Load the pretrained model and preprocessor\n",
    "    XGB_pre_trained_model = load_object(model_path + \"XGRegressorModel_v2.pkl\")\n",
    "    preprocessor = load_object(preprocessor_path)\n",
    "\n",
    "    # Load fresh data\n",
    "    fresh_data_path = \"../datasets/diamonds/fresh_data.csv\"\n",
    "    fresh_data = pd.read_csv(fresh_data_path)\n",
    "\n",
    "    # Process the fresh data\n",
    "    X_new_preprocessed, y_new = preprocess_data(fresh_data, preprocessor)\n",
    "\n",
    "    # Fine-tune the XGBoost model on the new data\n",
    "    dnew = xgb.DMatrix(X_new_preprocessed, label=y_new)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'eta': 0.1,\n",
    "    }\n",
    "    params['nrounds'] = 0\n",
    "    updated_model = xgb.train(params, dnew, xgb_model=XGB_pre_trained_model, num_boost_round=0)\n",
    "\n",
    "    # Overwrite the old model with the updated model\n",
    "    save_object(model_path + f'XGRegressorModel_v2.pkl', updated_model)\n",
    "\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional aspect to consider\n",
    "\n",
    "+ Log the pipeline execution to monitor performance and any potential errors.\n",
    "+ Implement a validation step to evaluate the performance of the updated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline v2 \n",
    "\n",
    "This version adds a log file and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='train_pipeline.log', level=logging.INFO)\n",
    "\n",
    "# Define the model and preprocessor paths\n",
    "model_path = \"./models/\"\n",
    "model_name = \"XGRegressorModel_v2\"\n",
    "preprocessor_path = \"./models/preprocessor.pkl\"\n",
    "\n",
    "fresh_data_path = \"../datasets/diamonds/fresh_data.csv\"\n",
    "\n",
    "def train_pipeline( model_path = \"./models/\",\n",
    "                    model_name = \"XGRegressorModel_v2\",\n",
    "                    preprocessor_path = \"./models/preprocessor.pkl\",\n",
    "                    fresh_data_path = \"../datasets/diamonds/fresh_data.csv\"):\n",
    "    logging.info(\"Starting pipeline execution\")\n",
    "\n",
    "    # Load the pretrained model and preprocessor\n",
    "    XGB_pre_trained_model = load_object(model_path + model_name + \".pkl\")\n",
    "    preprocessor = load_object(preprocessor_path)\n",
    "\n",
    "    # Load fresh data\n",
    "    fresh_data = pd.read_csv(fresh_data_path)\n",
    "\n",
    "    # Process the fresh data\n",
    "    X_new_preprocessed, y_new = preprocess_data(fresh_data, preprocessor)\n",
    "\n",
    "    # Fine-tune the XGBoost model on the new data\n",
    "    dnew = xgb.DMatrix(X_new_preprocessed, label=y_new)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'eta': 0.1,\n",
    "    }\n",
    "    params['nrounds'] = 0\n",
    "    updated_model = xgb.train(params, dnew, xgb_model=XGB_pre_trained_model, num_boost_round=0)\n",
    "\n",
    "    # Evaluate the performance of the updated model\n",
    "    logging.info(\"Evaluating model performance\")\n",
    "    xgb_predictions = updated_model.predict(dnew)\n",
    "    xgb_rmse = mean_squared_error(y_new, xgb_predictions, squared=False)\n",
    "    xgb_r2 = r2_score(y_new, xgb_predictions)\n",
    "\n",
    "    logging.info(\"XGBoost Metrics:\")\n",
    "    logging.info(f\"Root Mean Squared Error (RMSE): {xgb_rmse:.2f}\")\n",
    "    logging.info(f\"R-squared (R2): {xgb_r2:.2f}\")\n",
    "    \n",
    "    # Overwrite the old model with the updated model\n",
    "    logging.info(\"Saving updated model\")\n",
    "    save_object(model_path + f'XGRegressorModel_v2.pkl', updated_model)\n",
    "\n",
    "    logging.info(\"Pipeline execution complete\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [20:53:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"nrounds\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline v3 \n",
    "\n",
    "This version improves the format of the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Set up the logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler(filename='pipeline.log', mode='a')\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# Define the model and preprocessor paths\n",
    "model_path = \"./models/\"\n",
    "model_name = \"XGRegressorModel_v2\"\n",
    "preprocessor_path = \"./models/preprocessor.pkl\"\n",
    "\n",
    "fresh_data_path = \"../datasets/diamonds/fresh_data.csv\"\n",
    "\n",
    "def train_pipeline( model_path = \"./models/\",\n",
    "                    model_name = \"XGRegressorModel_v2\",\n",
    "                    preprocessor_path = \"./models/preprocessor.pkl\",\n",
    "                    fresh_data_path = \"../datasets/diamonds/fresh_data.csv\"):\n",
    "    logger.info('Starting pipeline execution')\n",
    "\n",
    "    # Load the pretrained model and preprocessor\n",
    "    logger.info('Loading pretrained model and preprocessor')\n",
    "    XGB_pre_trained_model = load_object(model_path + model_name + \".pkl\")\n",
    "    preprocessor = load_object(preprocessor_path)\n",
    "\n",
    "    # Load fresh data\n",
    "    fresh_data = pd.read_csv(fresh_data_path)\n",
    "\n",
    "    # Process the fresh data\n",
    "    logger.info('Preprocessing fresh data')\n",
    "    X_new_preprocessed, y_new = preprocess_data(fresh_data, preprocessor)\n",
    "\n",
    "    # Fine-tune the XGBoost model on the new data\n",
    "    logger.info('Fine-tuning XGBoost model')\n",
    "    dnew = xgb.DMatrix(X_new_preprocessed, label=y_new)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'eta': 0.1,\n",
    "    }\n",
    "    params['nrounds'] = 0\n",
    "    updated_model = xgb.train(params, dnew, xgb_model=XGB_pre_trained_model, num_boost_round=0)\n",
    "\n",
    "    # Evaluate the performance of the updated model\n",
    "    logger.info(\"Evaluating model performance\")\n",
    "    xgb_predictions = updated_model.predict(dnew)\n",
    "    xgb_rmse = mean_squared_error(y_new, xgb_predictions, squared=False)\n",
    "    xgb_r2 = r2_score(y_new, xgb_predictions)\n",
    "\n",
    "    logger.info(\"XGBoost Metrics:\")\n",
    "    logger.info(f\"Root Mean Squared Error (RMSE): {xgb_rmse:.2f}\")\n",
    "    logger.info(f\"R-squared (R2): {xgb_r2:.2f}\")\n",
    "    \n",
    "    # Overwrite the old model with the updated model\n",
    "    logger.info('Overwriting old model with updated model')\n",
    "    save_object(model_path + f'XGRegressorModel_v2.pkl', updated_model)\n",
    "\n",
    "    logger.info('Pipeline execution completed')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
